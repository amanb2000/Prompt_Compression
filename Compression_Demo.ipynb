{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Prompt Compression\n",
    "\n",
    "To summarize text $\\mathbf x$, we want compressed $\\mathbf x' : P_{LM}(\\mathbf x\n",
    "| \\mathbf x')$ is extremely high. \n",
    "\n",
    "We can use a large LLM (e.g., GPT-4) to suggest shorter and shorter versions of\n",
    "the text $\\mathbf x'$ as we select the best one at each iteration as the prompt\n",
    "maximizing $P_{LM}(\\mathbf x | \\mathbf x')$. \n",
    "\n",
    "Acknowledgements: Discussion with Dr. Alessandro Achille, Prof. Stefano Soatto \n",
    "at AWS research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import box\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "OAI_KEY_PATH = \"OAI_KEY.txt\"\n",
    "ANTHROPIC_KEY_PATH = \"ANTHROPIC_KEY.txt\"\n",
    "HF_MODEL = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get openai key from OAI_KEY_PATH\n",
    "with open(ANTHROPIC_KEY_PATH, 'r') as f:\n",
    "    anthropic_key = f.read().strip()\n",
    "\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=anthropic_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0da3d1f97154a4bb9989570488b75c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_MODEL)\n",
    "model = model.eval() \n",
    "#move to cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_compress = \"\"\"Mathematics\n",
    "Mathematics is an area of knowledge that includes the topics of numbers,\n",
    "formulas and related structures, shapes and the spaces in which they are\n",
    "contained, and quantities and their changes. These topics are represented in\n",
    "modern mathematics with the major subdisciplines of number theory,[1]\n",
    "algebra,[2] geometry,[1] and analysis,[3] respectively. There is no general\n",
    "consensus among mathematicians about a common definition for their academic\n",
    "discipline.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Hello Claude this is Aman. I'm building this system to\n",
    "compress text with small ~7b param language models. For text x, you're gonna\n",
    "produce a compressed version such that P(x | x') is maximized while\n",
    "minimizing the length of x'. You can use any prompting strategies you want. The\n",
    "user will give you the text x and you will respond with the compressed version.\n",
    "Note that the compressed version must be smaller than the input.\n",
    "\n",
    "\n",
    "REMEMBER TO DELIMIT YOUR COMPRESSED RESPONSE WITH <c> AND </c>!!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# remove any {, } characters\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m compressed \u001b[38;5;241m=\u001b[39m compress_text(text_to_compress, \u001b[43mclient\u001b[49m, system_prompt) \n\u001b[1;32m     42\u001b[0m compressed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompressed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m compressed\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "def compress_text(text_to_compress, client, system_prompt_, **kwargs): \n",
    "    \"\"\" Compresses text using anthropic's claude-3-haiku-20240307 model. \n",
    "    text_to_compress: str\n",
    "    client: anthropic.Anthropic() \n",
    "    system_prompt: str, guide for compression (includes <c> </c> spec), expecting \n",
    "        kwargs dict map for adding details.\n",
    "    *args: list[str], used with system_prompt.format() to produce final message.\n",
    "    \"\"\"\n",
    "    system_prompt = system_prompt_.format(**kwargs)\n",
    "    print(\"Sending API request at \", time.now())\n",
    "    _message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.9,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": text_to_compress\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"Received API response at \", time.now())\n",
    "    # print(_message)\n",
    "    content = _message.content\n",
    "    # print(content)\n",
    "    text = content[0].text\n",
    "\n",
    "    pattern = r\"<c>(.*?)<\\/c>\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    retval = matches[0]\n",
    "\n",
    "    # remove any {, } characters\n",
    "    return retval.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "\n",
    "compressed = compress_text(text_to_compress, client, system_prompt) \n",
    "compressed = f\"{compressed}\"\n",
    "compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6203409433364868"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_compressed(original, compressed, model, tokenizer): \n",
    "    \"\"\" - log P(x_p | x)\n",
    "    original: str \n",
    "    compressed: str \n",
    "    model: HF transformer \n",
    "    tokenizer: HF tokenizer\n",
    "    \"\"\"\n",
    "    x = original \n",
    "    x_p = compressed\n",
    "    x_ids = tokenizer(x).input_ids # list[int]\n",
    "    x_p_ids = tokenizer(x_p).input_ids # list[int]\n",
    "\n",
    "    label_ids = [-100 for _ in range(len(x_p_ids))] + x_ids # list[int]\n",
    "    input_ids = x_p_ids + x_ids # list[int]\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': torch.tensor(input_ids).unsqueeze(0).to(model.device),\n",
    "        'labels': torch.tensor(label_ids).unsqueeze(0).to(model.device),\n",
    "    }\n",
    "    # run thru model \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_dict)\n",
    "        loss = outputs.loss\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "score_compressed(original=f\"\\n\\nUNCOMPRESSED VERSION: \\n\\n\"+text_to_compress, \n",
    "                 compressed=\"\\n\\nCOMPRESSED VERSION: \\n\\n\"+compressed, \n",
    "                 model=model, \n",
    "                 tokenizer=tokenizer)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_compress_prompt = \"\"\"Hello Claude this is Aman. I'm building this system to\n",
    "compress text with small ~7b param language models. For text x, you're gonna\n",
    "produce a compressed version such that P(x | x') is maximized while\n",
    "minimizing the length of x'. You can use any prompting strategies you want. The\n",
    "user will give you the text x and you will respond with the compressed version.\n",
    "Note that the compressed version must be smaller than the input.\n",
    "\n",
    "Length (chars) of original (below): {original_len}\n",
    "Length (chars) of compressed sequence: {max_compressed_len}\n",
    "\n",
    "For context, here's one of the best ones to date: \n",
    "\n",
    "<c>\n",
    "{best_prompt}\n",
    "</c>\n",
    "\n",
    "REMEMBER TO DELIMIT YOUR COMPRESSED RESPONSE WITH <c> AND </c>!!\n",
    "\n",
    "Anyway, here's the original text: \n",
    "\"\"\"\n",
    "\n",
    "char_compress_kwargs = {\n",
    "    \"original_len\": -1, \n",
    "    \"max_compressed_len\": -1,\n",
    "    \"best_prompt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_compressed_prompt(original, \n",
    "                             client, \n",
    "                             model, \n",
    "                             tokenizer,\n",
    "                             prompt, \n",
    "                             prompt_kwargs,\n",
    "                             max_compressed_len=0.3, \n",
    "                             num_evolutions=10, \n",
    "                             pool_size=10): \n",
    "    \"\"\"\n",
    "    args: \n",
    "        original: str, original text to compress. \n",
    "        client: anthropic.Anthropic()\n",
    "        model: HF transformer \n",
    "        tokenizer: HF tokenizer\n",
    "        prompt: str, system prompt for compression to be formatted with prompt_kwargs\n",
    "        prompt_kwargs: dict{str: str}, kwargs with which to format prompt\n",
    "        max_compressed_len: int for max compressed length, or float for fraction of original. Default=0.3\n",
    "        num_evolutions: number of rounds of Claude calls to optimize pool \n",
    "        pool_size: pool of prompts to keep \n",
    "    \"\"\"\n",
    "    print(\"Length of original: \", len(original))\n",
    "    if type(max_compressed_len) == float: \n",
    "        assert max_compressed_len <= 1 and max_compressed_len >= 0\n",
    "        max_compressed_len = round(max_compressed_len* len(original))\n",
    "    assert max_compressed_len <= len(original) and max_compressed_len > 0\n",
    "\n",
    "    # initialize pool by calling compress_text(text_to_compress, client, system_prompt_, **kwargs) \n",
    "    pool = [] # list[str] of compressed prompts\n",
    "    for i in range(pool_size): \n",
    "        compressed_i = compress_text(original, client, prompt, **prompt_kwargs)\n",
    "        pool.append(compressed_i)\n",
    "        print(f\"Compressed {i} length: {len(compressed_i)}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # score_compressed(original=f\"\\n\\nUNCOMPRESSED VERSION: \\n\\n\"+text_to_compress, \n",
    "    #                 compressed=\"\\n\\nCOMPRESSED VERSION: \\n\\n\"+compressed, \n",
    "    #                 model=model, \n",
    "    #                 tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# call evo func\n",
    "evolve_compressed_prompt(original = text_to_compress, \n",
    "                        client = client, \n",
    "                        model = model, \n",
    "                        tokenizer = tokenizer,\n",
    "                        prompt = char_compress_prompt, \n",
    "                        prompt_kwargs = char_compress_kwargs,\n",
    "                        max_compressed_len=0.3, \n",
    "                        num_evolutions=10, \n",
    "                        pool_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
